<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Ensembling ConvNets using Keras | Maxim Mikhaylov</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
  </head>

  <body>
    <div class="container">
    <nav>
    <ul class="menu">
      <li> <a href="/">Max Mikhaylov</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/contact/">Contact</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
    </ul>
    </nav>
    </div>
    <hr/>
    <div class="container">


<div class="article-meta">
  <h1><span class="title">Ensembling ConvNets using Keras</span></h1>
  
  <h4 class="date"><span title="Publishing date." style="color: #000">December 13, 2017</span> - <span title="Date of the last major modification to this page." style="color: #000">February 20, 2019</spn></h4>
  
</div>

<main>
  <blockquote>
<p>This article was <a href="https://towardsdatascience.com/ensembling-convnets-using-keras-237d429157eb">originally pusblished</a> on Medium.com together with Towards Data Science.</p>
</blockquote>
<p><img src="/ensembling-convnets-using-keras/ensemble.jpeg" alt="Ensembling"></p>
<h1 id="introduction">Introduction</h1>
<blockquote>
<p>&ldquo;In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.&rdquo; [<a href="https://en.wikipedia.org/wiki/Ensemble_learning">1</a>]</p>
</blockquote>
<p>The main motivation for using an ensemble is to find a hypothesis that is not necessarily contained within the hypothesis space of the models from which it is built. Empirically, ensembles tend to yield better results when there is a significant diversity among the models. [<a href="http://jair.org/papers/paper614.html">2</a>]</p>
<h1 id="motivation">Motivation</h1>
<p>If you look at results of any big machine learning competition, you will most likely find that the top results are achieved by an ensemble of models rather than a single model. For instance, the top-scoring single model architecture at <a href="http://www.image-net.org/challenges/LSVRC/2015/results">ILSVRC2015</a> is on place 13. Places 1-12 are taken by various ensembles.</p>
<p>I haven&rsquo;t seen a tutorial or documentation on how to use several neural networks in an ensemble, so I decided to share the way I do it. I will be using <a href="https://keras.io/">Keras</a>, specifically its <a href="https://keras.io/models/model/">Functional API</a>, to recreate three small CNNs (compared to ResNet50, Inception etc.) from relatively well-known papers. I will train each model separately on <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a> training dataset. [<a href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">3</a>] Then each model will be evaluated using the test set. After that, I will put all three models in an ensemble and evaluate it. It is expected that the ensemble will perform better on a test set that any single model in the ensemble separately.</p>
<p>There are many different types of ensembles; stacking is one of them. It is one of the more general types and can theoretically represent any other ensemble technique. Stacking involves training a learning algorithm to combine the predictions of several other learning algorithms. [<a href="https://en.wikipedia.org/wiki/Ensemble_learning#Stacking">1</a>] For the sake of this example, I will use one of the simplest forms of Stacking, which involves taking an average of outputs of models in the ensemble. Since averaging doesn&rsquo;t take any parameters, there is no need to train this ensemble (only its models).</p>
<figure><img src="/ensembling-convnets-using-keras/ensemble.png"/><figcaption>
            <h4>Ensemble diagram</h4>
        </figcaption>
</figure>

<h2 id="preparing-the-data">Preparing the data</h2>
<p>First, import dependencies.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00f">from</span> keras.models <span style="color:#00f">import</span> Model, Input
</span></span><span style="display:flex;"><span><span style="color:#00f">from</span> keras.layers <span style="color:#00f">import</span> Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Activation, Average
</span></span><span style="display:flex;"><span><span style="color:#00f">from</span> keras.utils <span style="color:#00f">import</span> to_categorical
</span></span><span style="display:flex;"><span><span style="color:#00f">from</span> keras.losses <span style="color:#00f">import</span> categorical_crossentropy
</span></span><span style="display:flex;"><span><span style="color:#00f">from</span> keras.callbacks <span style="color:#00f">import</span> ModelCheckpoint, TensorBoard
</span></span><span style="display:flex;"><span><span style="color:#00f">from</span> keras.optimizers <span style="color:#00f">import</span> Adam
</span></span><span style="display:flex;"><span><span style="color:#00f">from</span> keras.datasets <span style="color:#00f">import</span> cifar10
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00f">import</span> numpy <span style="color:#00f">as</span> np
</span></span></code></pre></div><p>I am using CIFAR-10, since it is relatively easy to find papers describing architectures that work well on this dataset. Using a popular dataset also makes this example easily reproducible.</p>
<p>Here the dataset is imported. Both train and test image data is normalized. The training label vector is converted to a one-hot-matrix. Don&rsquo;t need to convert the test label vector, since it won&rsquo;t be used during training.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>(x_train, y_train), (x_test, y_test) = cifar10.load_data()
</span></span><span style="display:flex;"><span>x_train = x_train / 255.
</span></span><span style="display:flex;"><span>x_test = x_test / 255.
</span></span><span style="display:flex;"><span>y_train = to_categorical(y_train, num_classes=10)
</span></span></code></pre></div><p>The dataset consists of 60000 32x32 RGB images from 10 classes. 50000 images are used for training/validation and the other 10000 for testing.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(<span style="color:#a31515">&#39;x_train shape: </span><span style="color:#a31515">{}</span><span style="color:#a31515"> | y_train shape: </span><span style="color:#a31515">{}</span><span style="color:#a31515">\n</span><span style="color:#a31515">x_test shape : </span><span style="color:#a31515">{}</span><span style="color:#a31515"> | y_test shape : </span><span style="color:#a31515">{}</span><span style="color:#a31515">&#39;</span>.format(x_train.shape, y_train.shape,
</span></span><span style="display:flex;"><span>                                                                                          x_test.shape, y_test.shape))
</span></span></code></pre></div><pre><code>x_train shape: (50000, 32, 32, 3) | y_train shape: (50000, 10)
x_test shape : (10000, 32, 32, 3) | y_test shape : (10000, 1)
</code></pre>
<p>Since all three models work with the data of the same shape, it makes sense to define a single input layer that will be used by every model.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>input_shape = x_train[0,:,:,:].shape
</span></span><span style="display:flex;"><span>model_input = Input(shape=input_shape)
</span></span></code></pre></div><h1 id="first-model-convpool-cnn-c">First model: ConvPool-CNN-C</h1>
<p>The first model that I am going to train is ConvPool-CNN-C [<a href="https://arxiv.org/abs/1412.6806">4</a>]. Its descritption appears on page 4 of the linked paper.</p>
<p>The model is pretty straightforward. It features a common pattern where several convolutional layers are followed by a pooling layer. The only thing about this model that might be unfamiliar to some people is its final layers. Instead of using several fully-connected layers, a global average pooling layer is used.</p>
<p>Here is a brief overview of how global pooling layer works. The last convolutional layer <code>Conv2D(10, (1, 1))</code> outputs 10 feature maps corresponding to ten output classes. Then the <code>GlobalAveragePooling2D()</code> layer computes spatial average of these 10 feature maps, which means that its output is just a vector with a lenght 10. After that, a softmax activation is applied to that vector. As you can see, this method is in some way analogous to using FC layers at the top of the model. You can read more about global pooling layers and their advantages in <a href="https://arxiv.org/abs/1312.4400">5</a> paper.</p>
<p>One important thing to note: there&rsquo;s no activation function applied to the output of the final <code>Conv2D(10, (1, 1))</code> layer, since the output of this layer has to go through <code>GlobalAveragePooling2D()</code> first.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00f">def</span> conv_pool_cnn(model_input):
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    x = Conv2D(96, kernel_size=(3, 3), activation=<span style="color:#a31515">&#39;relu&#39;</span>, padding = <span style="color:#a31515">&#39;same&#39;</span>)(model_input)
</span></span><span style="display:flex;"><span>    x = Conv2D(96, (3, 3), activation=<span style="color:#a31515">&#39;relu&#39;</span>, padding = <span style="color:#a31515">&#39;same&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(96, (3, 3), activation=<span style="color:#a31515">&#39;relu&#39;</span>, padding = <span style="color:#a31515">&#39;same&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = MaxPooling2D(pool_size=(3, 3), strides = 2)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(192, (3, 3), activation=<span style="color:#a31515">&#39;relu&#39;</span>, padding = <span style="color:#a31515">&#39;same&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(192, (3, 3), activation=<span style="color:#a31515">&#39;relu&#39;</span>, padding = <span style="color:#a31515">&#39;same&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(192, (3, 3), activation=<span style="color:#a31515">&#39;relu&#39;</span>, padding = <span style="color:#a31515">&#39;same&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = MaxPooling2D(pool_size=(3, 3), strides = 2)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(192, (3, 3), activation=<span style="color:#a31515">&#39;relu&#39;</span>, padding = <span style="color:#a31515">&#39;same&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(192, (1, 1), activation=<span style="color:#a31515">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(10, (1, 1))(x)
</span></span><span style="display:flex;"><span>    x = GlobalAveragePooling2D()(x)
</span></span><span style="display:flex;"><span>    x = Activation(activation=<span style="color:#a31515">&#39;softmax&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    model = Model(model_input, x, name=<span style="color:#a31515">&#39;conv_pool_cnn&#39;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#00f">return</span> model
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>conv_pool_cnn_model = conv_pool_cnn(model_input)
</span></span></code></pre></div><p>For simplicity&rsquo;s sake, each model is compiled and trained using the same parameters. Using 20 epochs with a batch size of 32 (1250 steps per epoch) seems sufficient for any of the three models to get to some local minima. Randomly chosen 20% of the training dataset is used for validation.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00f">def</span> compile_and_train(model, num_epochs): 
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=[<span style="color:#a31515">&#39;acc&#39;</span>]) 
</span></span><span style="display:flex;"><span>    filepath = <span style="color:#a31515">&#39;weights/&#39;</span> + model.name + <span style="color:#a31515">&#39;.</span><span style="color:#a31515">{epoch:02d}</span><span style="color:#a31515">-</span><span style="color:#a31515">{loss:.2f}</span><span style="color:#a31515">.hdf5&#39;</span>
</span></span><span style="display:flex;"><span>    checkpoint = ModelCheckpoint(filepath, monitor=<span style="color:#a31515">&#39;loss&#39;</span>, verbose=0, save_weights_only=<span style="color:#00f">True</span>,
</span></span><span style="display:flex;"><span>                                                 save_best_only=<span style="color:#00f">True</span>, mode=<span style="color:#a31515">&#39;auto&#39;</span>, period=1)
</span></span><span style="display:flex;"><span>    tensor_board = TensorBoard(log_dir=<span style="color:#a31515">&#39;logs/&#39;</span>, histogram_freq=0, batch_size=32)
</span></span><span style="display:flex;"><span>    history = model.fit(x=x_train, y=y_train, batch_size=32, 
</span></span><span style="display:flex;"><span>                     epochs=num_epochs, verbose=1, callbacks=[checkpoint, tensor_board], validation_split=0.2)
</span></span><span style="display:flex;"><span>    <span style="color:#00f">return</span> history
</span></span></code></pre></div><p>It takes about 1 min to train this and the next model for one epoch using a single Tesla K80 GPU. Training might take a while if you are using a CPU.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>_ = compile_and_train(conv_pool_cnn_model, num_epochs=20)
</span></span></code></pre></div><p>The model achieves ~79% validation accuracy (the plots show validation accuracy/loss vs epoch #).</p>
<p><img src="/ensembling-convnets-using-keras/tensorboard_plots/11.png" alt="Accuracy plot"></p>
<p>One simple way to evaluate the model is to calculate the error rate on the test set.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00f">def</span> evaluate_error(model):
</span></span><span style="display:flex;"><span>    pred = model.predict(x_test, batch_size = 32)
</span></span><span style="display:flex;"><span>    pred = np.argmax(pred, axis=1)
</span></span><span style="display:flex;"><span>    pred = np.expand_dims(pred, axis=1) <span style="color:#008000"># make same shape as y_test</span>
</span></span><span style="display:flex;"><span>    error = np.sum(np.not_equal(pred, y_test)) / y_test.shape[0]    
</span></span><span style="display:flex;"><span>    <span style="color:#00f">return</span> error
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>evaluate_error(conv_pool_cnn_model)
</span></span></code></pre></div><pre><code>0.2414
</code></pre>
<h1 id="second-model-all-cnn-c">Second model: ALL-CNN-C</h1>
<p>The next CNN, ALL-CNN-C, comes from the same paper [<a href="https://arxiv.org/abs/1412.6806">4</a>]. This model is very similar to the previous one. Really, the only difference is that convolutional layers with a stride of 2 are used in place of max pooling layers. Again, note that there is no activation function used immediately after the <code>Conv2D(10, (1, 1))</code> layer. The model will fail to train if a <code>relu</code> activation is used immediately after that layer.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00f">def</span> all_cnn(model_input):
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    x = Conv2D(96, kernel_size=(3, 3), activation=<span style="color:#a31515">&#39;relu&#39;</span>, padding = <span style="color:#a31515">&#39;same&#39;</span>)(model_input)
</span></span><span style="display:flex;"><span>    x = Conv2D(96, (3, 3), activation=<span style="color:#a31515">&#39;relu&#39;</span>, padding = <span style="color:#a31515">&#39;same&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(96, (3, 3), activation=<span style="color:#a31515">&#39;relu&#39;</span>, padding = <span style="color:#a31515">&#39;same&#39;</span>, strides = 2)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(192, (3, 3), activation=<span style="color:#a31515">&#39;relu&#39;</span>, padding = <span style="color:#a31515">&#39;same&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(192, (3, 3), activation=<span style="color:#a31515">&#39;relu&#39;</span>, padding = <span style="color:#a31515">&#39;same&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(192, (3, 3), activation=<span style="color:#a31515">&#39;relu&#39;</span>, padding = <span style="color:#a31515">&#39;same&#39;</span>, strides = 2)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(192, (3, 3), activation=<span style="color:#a31515">&#39;relu&#39;</span>, padding = <span style="color:#a31515">&#39;same&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(192, (1, 1), activation=<span style="color:#a31515">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(10, (1, 1))(x)
</span></span><span style="display:flex;"><span>    x = GlobalAveragePooling2D()(x)
</span></span><span style="display:flex;"><span>    x = Activation(activation=<span style="color:#a31515">&#39;softmax&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    model = Model(model_input, x, name=<span style="color:#a31515">&#39;all_cnn&#39;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#00f">return</span> model
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>all_cnn_model = all_cnn(model_input)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>_ = compile_and_train(all_cnn_model, num_epochs=20)
</span></span></code></pre></div><p>The model converges to ~75% validation accuracy.</p>
<p><img src="/ensembling-convnets-using-keras/tensorboard_plots/22.png" alt="Accuracy plot"></p>
<p>Since two models are very similar to each other, it is expected that the error rate doesn&rsquo;t differ much.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>evaluate_error(all_cnn_model)
</span></span></code></pre></div><pre><code>0.26090000000000002
</code></pre>
<h1 id="third-model-network-in-network-cnn">Third Model: Network In Network CNN</h1>
<p>The third CNN is Network in Network CNN [<a href="https://arxiv.org/abs/1312.4400">5</a>]. This is a CNN from the paper that introduced global pooling layers. It&rsquo;s smaller than previous two models, therefore is much faster to train. No <code>relu</code> after the final convolutional layer!</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00f">def</span> nin_cnn(model_input):
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#008000">#mlpconv block 1</span>
</span></span><span style="display:flex;"><span>    x = Conv2D(32, (5, 5), activation=<span style="color:#a31515">&#39;relu&#39;</span>,padding=<span style="color:#a31515">&#39;valid&#39;</span>)(model_input)
</span></span><span style="display:flex;"><span>    x = Conv2D(32, (1, 1), activation=<span style="color:#a31515">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(32, (1, 1), activation=<span style="color:#a31515">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = MaxPooling2D((2,2))(x)
</span></span><span style="display:flex;"><span>    x = Dropout(0.5)(x)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#008000">#mlpconv block2</span>
</span></span><span style="display:flex;"><span>    x = Conv2D(64, (3, 3), activation=<span style="color:#a31515">&#39;relu&#39;</span>,padding=<span style="color:#a31515">&#39;valid&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(64, (1, 1), activation=<span style="color:#a31515">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(64, (1, 1), activation=<span style="color:#a31515">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = MaxPooling2D((2,2))(x)
</span></span><span style="display:flex;"><span>    x = Dropout(0.5)(x)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#008000">#mlpconv block3</span>
</span></span><span style="display:flex;"><span>    x = Conv2D(128, (3, 3), activation=<span style="color:#a31515">&#39;relu&#39;</span>,padding=<span style="color:#a31515">&#39;valid&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(32, (1, 1), activation=<span style="color:#a31515">&#39;relu&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    x = Conv2D(10, (1, 1))(x)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    x = GlobalAveragePooling2D()(x)
</span></span><span style="display:flex;"><span>    x = Activation(activation=<span style="color:#a31515">&#39;softmax&#39;</span>)(x)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    model = Model(model_input, x, name=<span style="color:#a31515">&#39;nin_cnn&#39;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#00f">return</span> model
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>nin_cnn_model = nin_cnn(model_input)
</span></span></code></pre></div><p>This model trains much faster -  15 seconds per epoch on my machine.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>_ = compile_and_train(nin_cnn_model, num_epochs=20)
</span></span></code></pre></div><p>The model achieves ~65% validation accuracy.</p>
<p><img src="/ensembling-convnets-using-keras/tensorboard_plots/33.png" alt="Accuracy plot"></p>
<p>This is more simple than the other two, so the error rate is a bit higher.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>evaluate_error(nin_cnn_model)
</span></span></code></pre></div><pre><code>0.31640000000000001
</code></pre>
<h1 id="three-model-ensemble">Three Model Ensemble</h1>
<p>Now all three models will be combined in an ensemble.</p>
<p>Here, all three models are reinstantiated and the best saved weights are loaded.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>conv_pool_cnn_model = conv_pool_cnn(model_input)
</span></span><span style="display:flex;"><span>all_cnn_model = all_cnn(model_input)
</span></span><span style="display:flex;"><span>nin_cnn_model = nin_cnn(model_input)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>conv_pool_cnn_model.load_weights(<span style="color:#a31515">&#39;weights/conv_pool_cnn.29-0.10.hdf5&#39;</span>)
</span></span><span style="display:flex;"><span>all_cnn_model.load_weights(<span style="color:#a31515">&#39;weights/all_cnn.30-0.08.hdf5&#39;</span>)
</span></span><span style="display:flex;"><span>nin_cnn_model.load_weights(<span style="color:#a31515">&#39;weights/nin_cnn.30-0.93.hdf5&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>models = [conv_pool_cnn_model, all_cnn_model, nin_cnn_model]
</span></span></code></pre></div><p>Ensemble model definition is very straightforward. It uses the same input layer thas is shared between all previous models. In the top layer, the ensemble computes the average of three models&rsquo; outputs by using <code>Average()</code> merge layer.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00f">def</span> ensemble(models, model_input):
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    outputs = [model.outputs[0] <span style="color:#00f">for</span> model <span style="color:#00f">in</span> models]
</span></span><span style="display:flex;"><span>    y = Average()(outputs)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    model = Model(model_input, y, name=<span style="color:#a31515">&#39;ensemble&#39;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#00f">return</span> model
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ensemble_model = ensemble(models, model_input)
</span></span></code></pre></div><p>As expected, the ensemble has a lower error rate than any single model.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>evaluate_error(ensemble_model)
</span></span></code></pre></div><pre><code>0.2049
</code></pre>
<h1 id="other-possible-ensembles">Other Possible Ensembles</h1>
<p>Just for completeness, we can check performance of ensembles that consist of 2 model combinations.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pair_A = [conv_pool_cnn_model, all_cnn_model]
</span></span><span style="display:flex;"><span>pair_B = [conv_pool_cnn_model, nin_cnn_model]
</span></span><span style="display:flex;"><span>pair_C = [all_cnn_model, nin_cnn_model]
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pair_A_ensemble_model = ensemble(pair_A, model_input)
</span></span><span style="display:flex;"><span>evaluate_error(pair_A_ensemble_model)
</span></span></code></pre></div><pre><code>0.21199999999999999
</code></pre>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pair_B_ensemble_model = ensemble(pair_B, model_input)
</span></span><span style="display:flex;"><span>evaluate_error(pair_B_ensemble_model)
</span></span></code></pre></div><pre><code>0.22819999999999999
</code></pre>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pair_C_ensemble_model = ensemble(pair_C, model_input)
</span></span><span style="display:flex;"><span>evaluate_error(pair_C_ensemble_model)
</span></span></code></pre></div><pre><code>0.2447
</code></pre>
<h1 id="conclusion">Conclusion</h1>
<p>To reiterate what was said in the introduction: every model has its own weaknesses. The reasoning behind using an ensemble is that by stacking different models representing different hypotheses about the data, we can find a better hypothesis that is not in the hypothesis space of the models from which the ensemble is built.</p>
<p>By using a very basic ensemble, a much lower error rate was achieved than when a single model was used. This proves effectiveness of ensembling.</p>
<p>Of course, there are some practical considerations to keep in mind when using an ensemble for your machine learning task. Since ensembling means stacking multiple models together, it also means that the input data needs to be forward-propagated for each model. This increases the amount of compute that needs to be performed and, consequently, evaluation (predicition) time. Increased evaluation time is not critical if you use an ensemble in research or in a Kaggle competition. However, it is a very critical factor when designing a commercial product. Another consideration is increased size of the final model which, again, might be a limiting factor for ensemble use in a commercial product.</p>
<hr>
<p>You can get the Jupyter notebook source code from my <a href="https://github.com/mmxmb/keras_ensemblng">GitHub</a>.</p>
<hr>
<h3 id="references">References</h3>
<ol>
<li><em>Ensemble Learning</em>. (n.d.). In Wikipedia. Retrieved December 12, 2017, from <a href="https://en.wikipedia.org/wiki/Ensemble_learning">https://en.wikipedia.org/wiki/Ensemble_learning</a></li>
<li>D. Opitz and R. Maclin (1999) <em>“Popular Ensemble Methods: An Empirical Study”</em>, Volume 11, pages 169–198 (available at <a href="http://jair.org/papers/paper614.html">http://jair.org/papers/paper614.html</a>)</li>
<li><a href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf"><em>Learning Multiple Layers of Features from Tiny Images</em></a>, Alex Krizhevsky, 2009.</li>
<li><a href="https://arxiv.org/abs/1412.6806v3">arXiv:1412.6806v3</a> [cs.LG]</li>
<li><a href="https://arxiv.org/abs/1312.4400v3">arXiv:1312.4400v3</a> [cs.NE]</li>
</ol>

</main>

  </div> 
  <footer>
  
  <hr/>
  <div class="container">
  <a href="https://github.com/mmxmb">Github</a> / <a href="/index.xml">RSS</a>
  
  </footer>
  </body>
  </div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

</html>

